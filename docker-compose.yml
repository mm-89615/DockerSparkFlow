x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: docker/airflow/Dockerfile
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_LOAD_EXAMPLES:-False}
    AIRFLOW__CORE__TEST_CONNECTION: ${AIRFLOW_TEST_CONNECTION:-"Enabled"}
    AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW_LOAD_DEFAULT_CONNECTIONS:-False}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}"
    AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW_LOGGING_LEVEL:-INFO}
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ${AIRFLOW_EXPOSE_CONFIG:-True}
    AIRFLOW__WEBSERVER__SECRET_KEY: a0B_2iP3mi8b9VlJP5XmQ1KY2VZE_yItCxRaDkhECv0=
  depends_on:
    postgres:
      condition: service_healthy
  volumes:
    - ./dags:/opt/airflow/dags
    - ./jobs:/opt/airflow/jobs
  healthcheck:
    interval: 10s
    timeout: 5s
    retries: 5
  restart: unless-stopped
  networks:
    spark-flow:

x-spark-common: &spark-common
  build:
    context: .
    dockerfile: docker/spark/Dockerfile
  volumes:
    - ./jobs:/opt/bitnami/spark/jobs
  environment:
    SPARK_RPC_AUTHENTICATION_ENABLED: no
    SPARK_RPC_ENCRYPTION_ENABLED: no
    SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
    SPARK_SSL_ENABLED: no
  healthcheck:
    interval: 10s
    timeout: 5s
    retries: 5
  networks:
    spark-flow:
  command: [ "/entrypoint.sh" ]

x-spark-worker-common: &spark-worker-common
  <<: *spark-common
  environment:
    SPARK_MODE: "worker"
    SPARK_MASTER_URL: "spark://spark-master:7077"
    SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-1}
    SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-2G}
  depends_on:
    - spark-master
  deploy:
    resources:
      limits:
        cpus: ${SPARK_WORKER_CORES:-1}
        memory: ${SPARK_WORKER_MEMORY:-2G}

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "${POSTGRES_PORT:-6432}:5432"
    volumes:
      - ./docker/pg_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}" ]
    networks:
      spark-flow:
    deploy:
      resources:
        limits:
          cpus: ${POSTGRES_CPU:-1}
          memory: ${POSTGRES_MEMORY:-2G}

  airflow-webserver:
    <<: *airflow-common
    ports:
      - "${AIRFLOW_WEB_PORT:-8080}:8080"
    healthcheck:
      test: [ "CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1" ]
    depends_on:
      - airflow-scheduled
    deploy:
      resources:
        limits:
          cpus: ${AIRFLOW_WEBSERVER_CPU:-1}
          memory: ${AIRFLOW_WEBSERVER_MEMORY:-2G}

  airflow-scheduled:
    <<: *airflow-common
    command: [ "airflow", "scheduler" ]
    healthcheck:
      test: [ "CMD", "airflow", "health" ]
    deploy:
      resources:
        limits:
          cpus: ${AIRFLOW_SCHEDULER_CPU:-1}
          memory: ${AIRFLOW_SCHEDULER_MEMORY:-2G}

  spark-master:
    <<: *spark-common
    environment:
      SPARK_MODE: "master"
    ports:
      - "${SPARK_MASTER_WEBUI_PORT:-9090}:8080"
      - "7077:7077"
    healthcheck:
      test: [ "CMD-SHELL", "curl --fail http://localhost:${SPARK_MASTER_WEBUI_PORT} || exit 1" ]
    deploy:
      resources:
        limits:
          cpus: ${SPARK_MASTER_CORES:-1}
          memory: ${SPARK_MASTER_MEMORY:-2G}

  spark-worker-1:
    <<: *spark-worker-common

  spark-worker-2:
    <<: *spark-worker-common


networks:
  spark-flow:
